# LLM Evaluation Framework - Week 1

A comprehensive framework for evaluating Large Language Models (LLMs) on domain-specific tasks, starting with the HumAID humanitarian crisis dataset.

## ğŸ¯ Project Overview

This project implements Week 1 of the Aleph Alpha LLM Evaluation track, focusing on:
- Dataset exploration and analysis
- Identifying real-world evaluation challenges
- Building foundation for custom evaluation pipelines

## ğŸ“ Project Structure

```
llm-evaluation-framework/
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package setup file
â”œâ”€â”€ .gitignore               # Git ignore file
â”œâ”€â”€ .env.example             # Environment variables template
â”‚
â”œâ”€â”€ config/                  # Configuration files
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py          # Global settings
â”‚   â””â”€â”€ datasets.yaml        # Dataset configurations
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/               # Data handling modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ downloader.py   # Dataset download logic
â”‚   â”‚   â”œâ”€â”€ explorer.py     # Dataset exploration
â”‚   â”‚   â””â”€â”€ analyzer.py     # Statistical analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/      # Visualization modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ plotter.py      # Plotting functions
â”‚   â”‚
â”‚   â””â”€â”€ utils/              # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py       # Logging configuration
â”‚       â””â”€â”€ helpers.py      # Helper functions
â”‚
â”œâ”€â”€ tests/                  # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_downloader.py
â”‚   â””â”€â”€ test_analyzer.py
â”‚
â””â”€â”€ main.py                 # Main entry point
```

## ğŸš€ Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/llm-evaluation-framework.git
cd llm-evaluation-framework
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

### Usage

Run the complete exploration pipeline:
```bash
python main.py
```

Advanced options:
```bash
python main.py --dataset "QCRI/HumAID-events"  # Different dataset
python main.py --download-only                  # Only download
python main.py --analyze-only                   # Skip download
python main.py --verbose                        # Verbose output
```

## ğŸ“Š Features

- **Automatic dataset discovery**: Detects columns, data types, and splits
- **Statistical analysis**: Word counts, character lengths, distributions
- **Label analysis**: Class balance, distribution metrics
- **Visualizations**: Comprehensive plots and charts
- **Reports**: Text, JSON, and visual reports

## ğŸ“ˆ Output

After running, find results in `outputs/`:
- `reports/`: Analysis reports and statistics
- `visualizations/`: Generated plots
- `data/`: Downloaded datasets

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit changes
4. Push to branch
5. Open a Pull Request

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- Aleph Alpha for the evaluation framework design
- QCRI for the HumAID dataset
- Hugging Face for dataset hosting
